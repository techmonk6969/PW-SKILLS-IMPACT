{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b20d94c-c4f0-42d0-9473-91aa4f662e02",
   "metadata": {},
   "source": [
    "Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with\n",
    "an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df3738-4359-4b97-b9df-e73b3f000d4b",
   "metadata": {},
   "source": [
    "**Probability Mass Function (PMF):**\n",
    "The Probability Mass Function (PMF) is a function that gives the probability of a discrete random variable taking on a specific value. It provides the probability distribution of a discrete random variable by assigning probabilities to each possible outcome. Mathematically, for a discrete random variable \\(X\\), the PMF is denoted as \\(P(X = x)\\), where \\(x\\) is a specific value, and it satisfies the following properties:\n",
    "\n",
    "1. \\(0 \\leq P(X = x) \\leq 1\\) for all \\(x\\).\n",
    "2. \\(\\sum P(X = x) = 1\\) over all possible values of \\(x\\).\n",
    "\n",
    "**Example of PMF:**\n",
    "Consider a fair six-sided die. The PMF for the outcome of rolling the die is given by:\n",
    "\n",
    "\\[ P(X = 1) = P(X = 2) = P(X = 3) = P(X = 4) = P(X = 5) = P(X = 6) = \\frac{1}{6} \\]\n",
    "\n",
    "Here, each outcome has an equal probability of \\(\\frac{1}{6}\\), and the PMF satisfies the properties mentioned above.\n",
    "\n",
    "**Probability Density Function (PDF):**\n",
    "The Probability Density Function (PDF) is a function that describes the likelihood of a continuous random variable falling within a specific range. It represents the probability distribution for continuous random variables. The PDF is denoted as \\(f(x)\\), and the probability of \\(X\\) falling within a certain interval \\([a, b]\\) is given by the integral of the PDF over that interval. Mathematically, for a continuous random variable \\(X\\):\n",
    "\n",
    "\\[ P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\,dx \\]\n",
    "\n",
    "**Example of PDF:**\n",
    "Consider the standard normal distribution. The PDF for this distribution is given by the bell-shaped curve, described by the normal distribution formula:\n",
    "\n",
    "\\[ f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}x^2} \\]\n",
    "\n",
    "This PDF describes the probability distribution of a continuous random variable with mean 0 and standard deviation 1. The area under the curve between any two points represents the probability of the variable falling within that range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f751f2c-3137-4759-adc1-ba01b2c2b1ee",
   "metadata": {},
   "source": [
    "Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feea4f-277c-42e3-b90d-7a1ec0769bd7",
   "metadata": {},
   "source": [
    "**Cumulative Density Function (CDF):**\n",
    "The Cumulative Density Function (CDF) is a function associated with a probability distribution, whether discrete or continuous. It provides the probability that a random variable takes on a value less than or equal to a given point. Mathematically, for a random variable \\(X\\), the CDF is denoted as \\(F(x)\\) and is defined as:\n",
    "\n",
    "\\[ F(x) = P(X \\leq x) \\]\n",
    "\n",
    "The CDF has the following properties:\n",
    "1. \\(0 \\leq F(x) \\leq 1\\) for all \\(x\\).\n",
    "2. \\(F(x)\\) is non-decreasing.\n",
    "3. As \\(x\\) approaches infinity, \\(F(x)\\) approaches 1.\n",
    "\n",
    "**Example of CDF:**\n",
    "Consider a fair six-sided die. The CDF for the outcome of rolling the die is given by:\n",
    "\n",
    "\\[ F(x) = P(X \\leq x) \\]\n",
    "\n",
    "- \\(F(1) = P(X \\leq 1) = \\frac{1}{6}\\)\n",
    "- \\(F(2) = P(X \\leq 2) = \\frac{1}{3}\\)\n",
    "- \\(F(3) = P(X \\leq 3) = \\frac{1}{2}\\)\n",
    "- \\(F(4) = P(X \\leq 4) = \\frac{2}{3}\\)\n",
    "- \\(F(5) = P(X \\leq 5) = \\frac{5}{6}\\)\n",
    "- \\(F(6) = P(X \\leq 6) = 1\\)\n",
    "\n",
    "**Why CDF is used?**\n",
    "1. **Probability Calculations:** The CDF allows for easy calculation of probabilities for a given range of values, as it provides cumulative probabilities.\n",
    "2. **Quantile Calculation:** CDF is used to find quantiles, which represent values below which a certain percentage of the data falls.\n",
    "3. **Understanding Distribution Characteristics:** CDF helps in visualizing the characteristics of a probability distribution, such as location, spread, and shape.\n",
    "\n",
    "In summary, the CDF is a valuable tool in probability theory and statistics, providing a convenient way to analyze and interpret the probabilities associated with random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2bccab-2c41-481a-a894-d9faa2e53000",
   "metadata": {},
   "source": [
    "Q3: What are some examples of situations where the normal distribution might be used as a model?\n",
    "Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25115bc6-f4cb-49fe-a810-7611c2664ef0",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution or bell curve, is a widely used probability distribution in various fields due to its mathematical properties and its occurrence in natural phenomena. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "1. **Height of Individuals:**\n",
    "   - The distribution of human heights is often modeled using a normal distribution. The heights of a large population tend to cluster around the mean, with fewer individuals at extreme heights.\n",
    "\n",
    "2. **IQ Scores:**\n",
    "   - Intelligence Quotient (IQ) scores are often assumed to follow a normal distribution. This assumption allows for the comparison of an individual's IQ to the general population.\n",
    "\n",
    "3. **Measurement Errors:**\n",
    "   - In many measurement processes, errors are normally distributed. This is useful for understanding and modeling the variability in measurements.\n",
    "\n",
    "4. **Financial Returns:**\n",
    "   - Stock returns and financial asset prices are often assumed to be normally distributed. This assumption forms the basis for various financial models.\n",
    "\n",
    "5. **Biological Parameters:**\n",
    "   - Biological parameters such as blood pressure, heart rate, and body temperature often exhibit a normal distribution in large populations.\n",
    "\n",
    "**Parameters of the Normal Distribution:**\n",
    "\n",
    "The normal distribution is characterized by two parameters: the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)). These parameters affect the shape and characteristics of the distribution:\n",
    "\n",
    "1. **Mean (\\(\\mu\\)):**\n",
    "   - The mean represents the central location or the average of the distribution.\n",
    "   - Shifting the mean to the right or left changes the location of the peak of the bell curve.\n",
    "\n",
    "2. **Standard Deviation (\\(\\sigma\\)):**\n",
    "   - The standard deviation measures the spread or variability of the distribution.\n",
    "   - A larger standard deviation results in a wider and flatter distribution, while a smaller standard deviation produces a narrower and taller distribution.\n",
    "\n",
    "The empirical rule, also known as the 68-95-99.7 rule, provides insights into the relationship between the mean and standard deviation:\n",
    "- Approximately 68% of the data falls within one standard deviation of the mean.\n",
    "- Approximately 95% falls within two standard deviations.\n",
    "- Approximately 99.7% falls within three standard deviations.\n",
    "\n",
    "Understanding these parameters allows for a comprehensive description and interpretation of the normal distribution in various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2550e4-6a83-4165-889d-3a0e8f251436",
   "metadata": {},
   "source": [
    "Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal\n",
    "Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae40cf-b7fc-41bc-80da-25dfe5a5b615",
   "metadata": {},
   "source": [
    "**Importance of Normal Distribution:**\n",
    "\n",
    "The normal distribution is of great importance in statistics and probability theory for several reasons:\n",
    "\n",
    "1. **Widespread Applicability:**\n",
    "   - The normal distribution is a versatile and widely applicable probability distribution. Many natural phenomena and processes in diverse fields exhibit a distribution that approximates a normal curve.\n",
    "\n",
    "2. **Central Limit Theorem:**\n",
    "   - The Central Limit Theorem states that the sum or average of a large number of independent and identically distributed random variables, regardless of their original distribution, tends to follow a normal distribution. This property is fundamental in statistical inference.\n",
    "\n",
    "3. **Statistical Inference:**\n",
    "   - Many statistical methods and tests are based on the assumption of normality. The normal distribution facilitates hypothesis testing, confidence interval estimation, and other inferential statistics.\n",
    "\n",
    "4. **Modeling and Prediction:**\n",
    "   - The normal distribution is used as a model for various random variables in fields such as finance, physics, biology, and engineering. This modeling helps in making predictions and understanding the behavior of systems.\n",
    "\n",
    "5. **Simplicity in Analysis:**\n",
    "   - The normal distribution is mathematically well-behaved, making calculations and analyses relatively straightforward. This simplicity eases the application of statistical techniques.\n",
    "\n",
    "6. **Standardization and Z-Scores:**\n",
    "   - The normal distribution is standardized with a mean of 0 and a standard deviation of 1. This standardization allows for the use of Z-scores, which represent the number of standard deviations a data point is from the mean. Z-scores provide a common scale for comparison.\n",
    "\n",
    "**Real-Life Examples of Normal Distribution:**\n",
    "\n",
    "1. **Height of Individuals:**\n",
    "   - Human heights often follow a normal distribution in large populations. The majority of people fall near the average height, with fewer individuals at extreme heights.\n",
    "\n",
    "2. **Exam Scores:**\n",
    "   - In educational settings, exam scores for a large group of students may exhibit a normal distribution. The distribution allows educators to understand the performance of students relative to the mean.\n",
    "\n",
    "3. **Blood Pressure:**\n",
    "   - Blood pressure readings in a population can be modeled using a normal distribution. Most individuals have blood pressure close to the mean, with fewer individuals having extremely high or low blood pressure.\n",
    "\n",
    "4. **IQ Scores:**\n",
    "   - Intelligence Quotient (IQ) scores are often assumed to follow a normal distribution. This assumption facilitates the comparison of an individual's cognitive abilities to the general population.\n",
    "\n",
    "5. **Measurement Errors:**\n",
    "   - Errors in measurements, such as those in scientific experiments or manufacturing processes, often follow a normal distribution. This assumption helps in understanding and modeling measurement variability.\n",
    "\n",
    "Understanding the normal distribution and its properties enhances the analysis and interpretation of data in various fields, contributing to the advancement of knowledge and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312f58c-ab90-43d5-85cd-7414b423a1db",
   "metadata": {},
   "source": [
    "Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli\n",
    "Distribution and Binomial Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f146df-3692-484c-977a-549df976ec44",
   "metadata": {},
   "source": [
    "**Bernoulli Distribution:**\n",
    "\n",
    "The Bernoulli distribution is a discrete probability distribution that models a random experiment with two possible outcomes: success and failure. It is named after the Swiss mathematician Jacob Bernoulli. The distribution is characterized by a single parameter, \\(p\\), which represents the probability of success.\n",
    "\n",
    "**Probability Mass Function (PMF) of Bernoulli Distribution:**\n",
    "\\[ P(X = k) = \\begin{cases} p & \\text{if } k = 1 \\\\ q = 1 - p & \\text{if } k = 0 \\end{cases} \\]\n",
    "\n",
    "**Example of Bernoulli Distribution:**\n",
    "Consider a single toss of a fair coin, where getting heads is considered a success (\\(k = 1\\)) and getting tails is a failure (\\(k = 0\\)). Let \\(p\\) be the probability of getting heads. The Bernoulli distribution for this scenario is given by:\n",
    "\\[ P(X = 1) = p \\]\n",
    "\\[ P(X = 0) = 1 - p \\]\n",
    "\n",
    "**Difference between Bernoulli Distribution and Binomial Distribution:**\n",
    "\n",
    "1. **Number of Trials:**\n",
    "   - **Bernoulli Distribution:** Describes a single trial or experiment with two possible outcomes (success or failure).\n",
    "   - **Binomial Distribution:** Describes the number of successes in a fixed number of independent and identical Bernoulli trials.\n",
    "\n",
    "2. **Parameters:**\n",
    "   - **Bernoulli Distribution:** Has a single parameter \\(p\\), representing the probability of success.\n",
    "   - **Binomial Distribution:** Has two parameters \\(n\\) (number of trials) and \\(p\\) (probability of success in a single trial).\n",
    "\n",
    "3. **Random Variable:**\n",
    "   - **Bernoulli Distribution:** The random variable can take values 0 (failure) or 1 (success).\n",
    "   - **Binomial Distribution:** The random variable represents the count of successes in \\(n\\) trials, taking values from 0 to \\(n\\).\n",
    "\n",
    "4. **Probability Mass Function (PMF):**\n",
    "   - **Bernoulli Distribution:** \\( P(X = k) = p^k (1 - p)^{1-k} \\) for \\(k = 0, 1\\).\n",
    "   - **Binomial Distribution:** \\( P(X = k) = \\binom{n}{k} p^k (1 - p)^{n-k} \\) for \\(k = 0, 1, ..., n\\).\n",
    "\n",
    "In summary, the Bernoulli distribution is a special case of the binomial distribution where there is only one trial (\\(n = 1\\)). The binomial distribution extends the concept to multiple independent trials with the same probability of success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f960a-ec9f-40b0-aa54-1a9d3ace886c",
   "metadata": {},
   "source": [
    "Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset\n",
    "is normally distributed, what is the probability that a randomly selected observation will be greater\n",
    "than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d4092-4e23-45d5-b8c6-03cd769725b6",
   "metadata": {},
   "source": [
    "To find the probability that a randomly selected observation from a normal distribution with a mean of 50 and a standard deviation of 10 will be greater than 60, we can use the standard normal distribution (z-score) and the cumulative distribution function (CDF). The formula for the z-score is:\n",
    "\n",
    "\\[ z = \\frac{{X - \\mu}}{{\\sigma}} \\]\n",
    "\n",
    "where:\n",
    "- \\(X\\) is the value in question (60 in this case),\n",
    "- \\(\\mu\\) is the mean of the distribution (50),\n",
    "- \\(\\sigma\\) is the standard deviation of the distribution (10).\n",
    "\n",
    "Calculate the z-score:\n",
    "\\[ z = \\frac{{60 - 50}}{{10}} = 1 \\]\n",
    "\n",
    "Now, use the standard normal distribution table or a calculator to find the probability that a z-score is greater than 1. In standard normal distribution tables, this value is typically given as \\(P(Z > 1)\\).\n",
    "\n",
    "\\[ P(X > 60) = P(Z > 1) \\]\n",
    "\n",
    "From standard normal distribution tables, you find that \\(P(Z > 1) \\approx 0.1587\\).\n",
    "\n",
    "Therefore, the probability that a randomly selected observation from the given normal distribution is greater than 60 is approximately \\(0.1587\\) or \\(15.87%\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084cc0bf-2884-4712-9526-11c60af37b2c",
   "metadata": {},
   "source": [
    "Q7: Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d47807-8efc-449b-be59-f76937519031",
   "metadata": {},
   "source": [
    "**Uniform Distribution:**\n",
    "\n",
    "The uniform distribution is a continuous probability distribution that describes a random variable with equal probability of taking any value within a specified range. In other words, every possible outcome in the range has the same likelihood of occurring. The probability density function (PDF) for a uniform distribution is constant over the range of possible values and zero outside that range.\n",
    "\n",
    "**Probability Density Function (PDF) of Uniform Distribution:**\n",
    "\\[ f(x) = \\frac{1}{b - a} \\]\n",
    "\n",
    "where \\(a\\) and \\(b\\) are the parameters defining the interval or range of possible values.\n",
    "\n",
    "**Example of Uniform Distribution:**\n",
    "\n",
    "Consider a random variable \\(X\\) representing the time it takes for a computer program to execute. If the program is designed to run for exactly 5 to 10 seconds, and any time within that interval is equally likely, then \\(X\\) follows a uniform distribution on the interval \\([5, 10]\\).\n",
    "\n",
    "**Probability Density Function for the Example:**\n",
    "\\[ f(x) = \\begin{cases} \\frac{1}{10-5} = \\frac{1}{5} & \\text{if } 5 \\leq x \\leq 10 \\\\ 0 & \\text{otherwise} \\end{cases} \\]\n",
    "\n",
    "In this example:\n",
    "- The probability of the program running for any specific time within \\([5, 10]\\) is the same.\n",
    "- The height of the PDF is constant over the interval, reflecting the uniform likelihood of any value within that range.\n",
    "- The total area under the PDF curve is equal to 1.\n",
    "\n",
    "The uniform distribution is commonly used in situations where each outcome within a given range is equally likely. It is a fundamental distribution in probability theory and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a832e8-fa7e-4dc9-98ce-65516089c85a",
   "metadata": {},
   "source": [
    "Q8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25edcba4-bf5e-465c-bfbf-0c31ee38923d",
   "metadata": {},
   "source": [
    "**Z-Score:**\n",
    "\n",
    "The z-score, also known as the standard score, is a measure of how many standard deviations a particular data point is from the mean of a distribution. It is a standardized score that allows for the comparison of values from different normal distributions. The formula for calculating the z-score for a data point \\(X\\) in a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is:\n",
    "\n",
    "\\[ z = \\frac{{X - \\mu}}{{\\sigma}} \\]\n",
    "\n",
    "where:\n",
    "- \\(X\\) is the individual data point,\n",
    "- \\(\\mu\\) is the mean of the distribution,\n",
    "- \\(\\sigma\\) is the standard deviation of the distribution.\n",
    "\n",
    "**Importance of Z-Score:**\n",
    "\n",
    "1. **Standardization:**\n",
    "   - Z-scores standardize data by expressing each data point in terms of standard deviations from the mean. This standardization allows for the comparison of data from different distributions.\n",
    "\n",
    "2. **Normal Distribution Comparison:**\n",
    "   - Z-scores are particularly useful when working with normal distributions. They allow us to determine the percentile rank or probability associated with a particular value in a normal distribution.\n",
    "\n",
    "3. **Identification of Outliers:**\n",
    "   - Z-scores help in identifying outliers in a dataset. Data points with extreme z-scores (far from 0) may indicate values that are unusual or need further investigation.\n",
    "\n",
    "4. **Data Transformation:**\n",
    "   - Z-scores are used in data transformation and normalization techniques. Transforming data to z-scores can help in making different datasets comparable and aids in statistical analyses.\n",
    "\n",
    "5. **Statistical Hypothesis Testing:**\n",
    "   - In hypothesis testing, z-scores play a crucial role. Critical values associated with certain levels of significance in hypothesis tests are often expressed in terms of z-scores.\n",
    "\n",
    "6. **Probability Calculation:**\n",
    "   - Z-scores are used to calculate probabilities associated with specific values in a normal distribution. This is particularly valuable in statistical inference and decision-making.\n",
    "\n",
    "7. **Benchmarking:**\n",
    "   - Z-scores provide a benchmark for comparing individual data points to the average of a distribution. Positive z-scores indicate values above the mean, while negative z-scores indicate values below the mean.\n",
    "\n",
    "In summary, the z-score is a powerful tool in statistics that facilitates the standardization and comparison of data across different distributions. It is widely used in various statistical applications, hypothesis testing, and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ce6b7-1c8e-48ce-8f23-048a032fe467",
   "metadata": {},
   "source": [
    "Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd6770-d90a-4ce7-a192-24011bc452be",
   "metadata": {},
   "source": [
    "**Central Limit Theorem (CLT):**\n",
    "\n",
    "The Central Limit Theorem is a fundamental concept in probability theory and statistics. It states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the original distribution of the individual variables. This holds true even if the original variables do not follow a normal distribution.\n",
    "\n",
    "**Key Points of the Central Limit Theorem:**\n",
    "\n",
    "1. **Large Sample Size:**\n",
    "   - The Central Limit Theorem applies as the sample size becomes sufficiently large. There is no strict rule for what constitutes a \"large\" sample size, but a common guideline is \\(n \\geq 30\\).\n",
    "\n",
    "2. **Independence:**\n",
    "   - The random variables must be independent. This means that the outcome of one variable does not influence the outcome of another.\n",
    "\n",
    "3. **Identical Distribution:**\n",
    "   - The random variables should be identically distributed, meaning they come from the same population and have the same probability distribution.\n",
    "\n",
    "4. **Convergence to Normal Distribution:**\n",
    "   - Regardless of the original distribution of the random variables, the distribution of the sample mean approaches a normal distribution as the sample size increases.\n",
    "\n",
    "**Significance of the Central Limit Theorem:**\n",
    "\n",
    "1. **Statistical Inference:**\n",
    "   - The Central Limit Theorem is a foundation for many statistical methods. It justifies the use of normal distribution-based statistical techniques even when the underlying population distribution is not normal.\n",
    "\n",
    "2. **Hypothesis Testing:**\n",
    "   - In hypothesis testing, especially when dealing with small sample sizes, the Central Limit Theorem allows for making inferences about population parameters using normal distribution-based methods.\n",
    "\n",
    "3. **Confidence Intervals:**\n",
    "   - The theorem is crucial for constructing confidence intervals for population parameters, such as the mean. It provides a basis for estimating the range within which the true parameter is likely to fall.\n",
    "\n",
    "4. **Sampling Distribution:**\n",
    "   - The Central Limit Theorem explains why the sampling distribution of the sample mean tends to be approximately normal, making it easier to make predictions about the behavior of sample statistics.\n",
    "\n",
    "5. **Population Distribution Irrespective:**\n",
    "   - The Central Limit Theorem is powerful because it allows statisticians to work with normal distribution properties, even when the underlying population distribution is unknown or not normal.\n",
    "\n",
    "6. **Quality Control and Process Improvement:**\n",
    "   - In fields such as manufacturing and quality control, the Central Limit Theorem is applied to analyze and improve processes based on sample means.\n",
    "\n",
    "7. **Practical Applications in Business and Science:**\n",
    "   - The theorem is widely used in various fields, including finance, biology, economics, and more, where statistical analyses and inferences are essential.\n",
    "\n",
    "In summary, the Central Limit Theorem is a cornerstone of statistical theory, providing a bridge between the properties of sample statistics and the characteristics of the underlying population. It has profound implications for the practice of statistics and its applications in various disciplines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f6ead-dd32-40f0-95f4-217cf34137da",
   "metadata": {},
   "source": [
    "Q10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96041641-7bce-4f2f-a677-c0df61ec78a2",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a powerful statistical concept, but it relies on certain assumptions for its application. The assumptions of the Central Limit Theorem are as follows:\n",
    "\n",
    "1. **Independence:**\n",
    "   - The random variables in the sample must be independent. The outcome of one variable should not influence the outcome of another. Independence is crucial for the validity of the CLT.\n",
    "\n",
    "2. **Identically Distributed:**\n",
    "   - The random variables should be identically distributed. This means that each variable in the sample comes from the same population and follows the same probability distribution.\n",
    "\n",
    "3. **Finite Variance:**\n",
    "   - The random variables should have a finite variance (\\(\\sigma^2\\)). The variance is a measure of how much the values in the sample differ from the mean. The existence of a finite variance is necessary for the CLT to work.\n",
    "\n",
    "4. **Sample Size:**\n",
    "   - The Central Limit Theorem is most effective and reliable for larger sample sizes. While there is no strict rule for what constitutes a \"large\" sample size, a common guideline is \\(n \\geq 30\\). Smaller sample sizes may still exhibit some degree of normality but might deviate more from a perfect normal distribution.\n",
    "\n",
    "It's important to note that the CLT is quite robust, and even if these assumptions are not perfectly met, the theorem can still provide reasonable approximations in many cases. However, for critical applications or when dealing with small sample sizes, adherence to these assumptions becomes more crucial. Additionally, there are variations of the CLT that relax some of these assumptions under specific conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
