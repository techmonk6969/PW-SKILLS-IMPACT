{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53701028-46c8-4e7a-b75b-77629687cfd1",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863f790-8a3d-4f18-817c-fac38a361ee1",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used to scale numerical features to a specific range, usually between 0 and 1. The purpose of Min-Max scaling is to standardize the range of independent variables or features of the data. The formula for Min-Max scaling is given by:\n",
    "\n",
    "�\n",
    "scaled\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "scaled\n",
    "X \n",
    "scaled\n",
    "​\n",
    "  is the scaled value,\n",
    "�\n",
    "X is the original value,\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value of the feature,\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea355383-2666-4423-8003-0884f394ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Feature1  Feature2\n",
      "0         2        15\n",
      "1         5        22\n",
      "2         8        35\n",
      "3        11        40\n",
      "4        14        50\n",
      "\n",
      "Scaled Data:\n",
      "   Feature1  Feature2\n",
      "0      0.00  0.000000\n",
      "1      0.25  0.200000\n",
      "2      0.50  0.571429\n",
      "3      0.75  0.714286\n",
      "4      1.00  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = {'Feature1': [2, 5, 8, 11, 14],\n",
    "        'Feature2': [15, 22, 35, 40, 50]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b735c-9791-489e-bc6f-505a7a9832be",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e872ae-7c30-4e34-a24f-82b04becbeb1",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Vector normalization or L2 normalization, is a feature scaling method that scales each feature of a data point by dividing it by the Euclidean norm (L2 norm) of the entire feature vector. The purpose of this technique is to ensure that the resulting vector has a Euclidean norm (length) of 1. It is particularly useful when the direction of the data points matters more than their magnitudes.\n",
    "\n",
    "The formula for Unit Vector scaling is given by:\n",
    "\n",
    "�\n",
    "scaled\n",
    "=\n",
    "�\n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "∥X∥ \n",
    "2\n",
    "​\n",
    " \n",
    "X\n",
    "​\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "scaled\n",
    "X \n",
    "scaled\n",
    "​\n",
    "  is the scaled vector,\n",
    "�\n",
    "X is the original vector,\n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "∥X∥ \n",
    "2\n",
    "​\n",
    "  is the L2 norm (Euclidean norm) of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a732a4b7-3963-4638-8d19-b7e8c694b43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[ 2  5]\n",
      " [ 8 11]\n",
      " [14 20]]\n",
      "\n",
      "Normalized Data:\n",
      "[[0.37139068 0.92847669]\n",
      " [0.5881717  0.80873608]\n",
      " [0.57346234 0.81923192]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[2, 5],\n",
    "                 [8, 11],\n",
    "                 [14, 20]])\n",
    "\n",
    "# Initialize the Normalizer with L2 normalization\n",
    "normalizer = Normalizer(norm='l2')\n",
    "\n",
    "# Transform the data\n",
    "normalized_data = normalizer.transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(normalized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b76e47-f6d7-4044-bd52-3b527db76550",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d9edc-e0a3-480d-bbf7-da93fdff1133",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving as much of the original variability as possible. The idea is to identify the principal components, which are linear combinations of the original features that capture the most significant information in the data.\n",
    "\n",
    "Here's a step-by-step explanation of how PCA works:\n",
    "\n",
    "Standardize the Data: Standardize the features to have zero mean and unit variance.\n",
    "\n",
    "Compute the Covariance Matrix: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the magnitude of variance in each direction.\n",
    "\n",
    "Sort Eigenvalues: Sort the eigenvalues in descending order and correspondingly arrange the eigenvectors.\n",
    "\n",
    "Select Principal Components: Choose the top \n",
    "�\n",
    "k eigenvectors based on the explained variance or a specified threshold. These eigenvectors become the new basis for the lower-dimensional space.\n",
    "\n",
    "Transform the Data: Project the original data onto the selected principal components to obtain the reduced-dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8856466e-fe4a-4b6b-b4dc-7b9ea3d1d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (150, 4)\n",
      "Reduced Data Shape: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the data\n",
    "X_standardized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Data Shape:\", X.shape)\n",
    "print(\"Reduced Data Shape:\", X_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80223c2-5210-4c23-a9d7-7b578c3298d0",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c24a8-013e-4d7b-ae99-c9295508b4e2",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) can be used for feature extraction, particularly when dealing with high-dimensional data. Feature extraction involves transforming the original features into a new set of features, often of lower dimensionality, while retaining the most important information. PCA achieves this by identifying the principal components, which are linear combinations of the original features.\n",
    "\n",
    "Here's the relationship between PCA and feature extraction:\n",
    "\n",
    "Dimensionality Reduction: PCA is primarily used for dimensionality reduction. It identifies the directions (principal components) in the data that capture the maximum variance. These principal components can be seen as new features that are linear combinations of the original features.\n",
    "\n",
    "Feature Extraction: In the context of PCA, the principal components themselves serve as the extracted features. Each principal component is a linear combination of the original features, and it represents a direction in the high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed5264e7-652d-41c0-a3d0-fe33b35dd30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2  Target\n",
      "0 -2.264703  0.480027       0\n",
      "1 -2.080961 -0.674134       0\n",
      "2 -2.364229 -0.341908       0\n",
      "3 -2.299384 -0.597395       0\n",
      "4 -2.389842  0.646835       0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the data\n",
    "X_standardized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# Apply PCA for feature extraction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "principal_components_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Concatenate the principal components with the target variable\n",
    "result_df = pd.concat([principal_components_df, pd.Series(y, name='Target')], axis=1)\n",
    "\n",
    "# Print the result\n",
    "print(result_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a763f82d-3b40-4058-b411-5b3eaa70124b",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913c191-49ed-4228-9ee2-0db57b786d00",
   "metadata": {},
   "source": [
    "\n",
    "Min-Max scaling is a technique used to normalize the range of independent variables or features of a dataset. It scales the values of the features to a specific range, usually between 0 and 1. This scaling is particularly useful when features have different units or scales, ensuring that each feature contributes equally to the model. Here's how you could use Min-Max scaling to preprocess the data for a recommendation system in the context of a food delivery service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1947c03-33e9-4aa4-b193-0713446d3a63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'food_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelivery_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a new DataFrame with only the selected features\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m selected_features_df \u001b[38;5;241m=\u001b[39m \u001b[43mfood_data\u001b[49m[features]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize MinMaxScaler\u001b[39;00m\n\u001b[1;32m     13\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'food_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assume you have a DataFrame 'food_data' with columns like 'price', 'rating', 'delivery_time', etc.\n",
    "\n",
    "# Extract the relevant features\n",
    "features = ['price', 'rating', 'delivery_time']\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "selected_features_df = food_data[features]\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the selected features using Min-Max scaling\n",
    "scaled_features = scaler.fit_transform(selected_features_df)\n",
    "\n",
    "# Create a new DataFrame with the scaled features\n",
    "scaled_features_df = pd.DataFrame(data=scaled_features, columns=features)\n",
    "\n",
    "# Concatenate the scaled features with the remaining columns in the original DataFrame\n",
    "preprocessed_data = pd.concat([scaled_features_df, food_data.drop(features, axis=1)], axis=1)\n",
    "\n",
    "# Now 'preprocessed_data' contains the original data with the selected features scaled using Min-Max scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d3d20-d6e2-48a6-b7f5-c74c4bb816c8",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4755552b-c278-4f88-8be9-7c8a5974d273",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that is commonly used to reduce the number of features in a dataset while retaining most of its original variability. Here's how you could use PCA to reduce the dimensionality of a dataset for predicting stock prices:\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Standardize the features: It's essential to standardize or normalize the features to ensure that each feature contributes equally to the PCA process. This step is crucial because PCA is sensitive to the scale of the features.\n",
    "Applying PCA:\n",
    "\n",
    "Use PCA to transform the standardized features into principal components. These components are linear combinations of the original features and are orthogonal to each other.\n",
    "Choose the number of principal components you want to retain. This decision is often based on the explained variance. You might aim to retain a certain percentage of the total variance (e.g., 95%).\n",
    "Fit the PCA model on the standardized features and transform the data to obtain the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9314003-6a04-486c-aeb4-ae3ffbf936b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stock_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create a new DataFrame with only the selected features\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m selected_features_df \u001b[38;5;241m=\u001b[39m \u001b[43mstock_data\u001b[49m[features]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Standardize the features\u001b[39;00m\n\u001b[1;32m     14\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stock_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume you have a DataFrame 'stock_data' with various financial and market trend features\n",
    "\n",
    "# Extract relevant features\n",
    "features = ['feature1', 'feature2', 'feature3', ...]\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "selected_features_df = stock_data[features]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(selected_features_df)\n",
    "\n",
    "# Choose the number of principal components to retain (e.g., n_components=3)\n",
    "n_components = 3\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA and transform the standardized features\n",
    "principal_components = pca.fit_transform(standardized_features)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pc_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, n_components + 1)])\n",
    "\n",
    "# Concatenate the principal components with the remaining columns in the original DataFrame\n",
    "preprocessed_data = pd.concat([pc_df, stock_data.drop(features, axis=1)], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7001a-5dae-46b4-aa32-8ce7fc0fb4cc",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21becfb2-1fc4-49b9-862d-2fbee44881c9",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on a dataset and transform the values to a range of -1 to 1, you can use the following formula:\n",
    "\n",
    "Scaled Value\n",
    "=\n",
    "Original Value\n",
    "−\n",
    "Min Value\n",
    "Max Value\n",
    "−\n",
    "Min Value\n",
    "×\n",
    "(\n",
    "New Max\n",
    "−\n",
    "New Min\n",
    ")\n",
    "+\n",
    "New Min\n",
    "Scaled Value= \n",
    "Max Value−Min Value\n",
    "Original Value−Min Value\n",
    "​\n",
    " ×(New Max−New Min)+New Min\n",
    "\n",
    "Let's apply this formula to the given dataset: [1, 5, 10, 15, 20], and scale the values to the range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25aa9a18-1c31-4f98-99b6-523a62c7d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Values: [ 1  5 10 15 20]\n",
      "Scaled Values (Min-Max Scaling to -1 to 1): [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given dataset\n",
    "original_values = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the new range\n",
    "new_min, new_max = -1, 1\n",
    "\n",
    "# Calculate Min-Max scaling\n",
    "min_value = np.min(original_values)\n",
    "max_value = np.max(original_values)\n",
    "\n",
    "scaled_values = (original_values - min_value) / (max_value - min_value) * (new_max - new_min) + new_min\n",
    "\n",
    "print(\"Original Values:\", original_values)\n",
    "print(\"Scaled Values (Min-Max Scaling to -1 to 1):\", scaled_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ee3df-6f0d-470c-a91d-b66a244ea6b6",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641372a-3752-4784-aabd-dfb07e7622a6",
   "metadata": {},
   "source": [
    "In Feature Extraction using Principal Component Analysis (PCA), the number of principal components to retain is a subjective decision and often involves a trade-off between retaining enough information and reducing the dimensionality of the dataset. The decision is typically based on the explained variance ratio.\n",
    "\n",
    "Here's a general approach to perform PCA and decide on the number of principal components to retain:\n",
    "\n",
    "Standardize the Data:\n",
    "It's important to standardize or normalize the features since PCA is sensitive to the scale of the data.\n",
    "\n",
    "Compute Covariance Matrix:\n",
    "Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues:\n",
    "Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance captured by each component.\n",
    "\n",
    "Sort Eigenvalues:\n",
    "Sort the eigenvalues in descending order.\n",
    "\n",
    "Choose the Number of Principal Components:\n",
    "Decide on the number of principal components to retain based on the explained variance ratio. A common approach is to choose a number that explains a significant portion (e.g., 95% or 99%) of the total variance.\n",
    "\n",
    "Project Data onto Principal Components:\n",
    "Project the original data onto the selected principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cdbac3b-7922-4b42-a427-7d055ca7b76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.95825757 0.04174243 0.        ]\n",
      "Cumulative Variance Ratio: [0.95825757 1.         1.        ]\n",
      "Number of Components to Retain: 1\n",
      "Principal Components:\n",
      " [[-0.29383087]\n",
      " [ 2.81565659]\n",
      " [-2.52182571]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = [[170, 65, 30, 1, 120],\n",
    "        [160, 55, 25, 0, 110],\n",
    "        [180, 75, 35, 1, 130]]\n",
    "        # ... (more rows)\n",
    "\n",
    "# Standardize the data\n",
    "scaled_data = StandardScaler().fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Choose the number of principal components\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "num_components_to_retain = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "# Apply PCA with the selected number of components\n",
    "pca = PCA(n_components=num_components_to_retain)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative Variance Ratio:\", cumulative_variance_ratio)\n",
    "print(\"Number of Components to Retain:\", num_components_to_retain)\n",
    "print(\"Principal Components:\\n\", principal_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489d81a-fc43-4329-a7ca-b5eeb70cf317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
