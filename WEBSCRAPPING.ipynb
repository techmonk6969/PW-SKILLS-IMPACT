{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89b6617-0845-4e0f-8d4c-e660e35ef5f5",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bdca84-506e-4349-9dfb-5000642d8ff7",
   "metadata": {},
   "source": [
    "**Web scraping** is a technique of extracting information or data from websites. It involves fetching the HTML content of a web page and then parsing and extracting the desired information from it. Web scraping is commonly used for various purposes such as data extraction, analysis, research, and automation.\n",
    "\n",
    "### Why is Web Scraping Used?\n",
    "\n",
    "1. **Data Extraction:**\n",
    "   - Web scraping is used to extract data from websites that do not provide a structured API or data feed. By analyzing the HTML structure of a webpage, one can programmatically extract specific information such as product prices, news articles, stock prices, and more.\n",
    "\n",
    "2. **Automated Data Collection:**\n",
    "   - Web scraping allows for the automated collection of data from multiple sources. Instead of manually copying and pasting information from different websites, web scraping scripts can be created to gather and consolidate data in a more efficient manner.\n",
    "\n",
    "3. **Market Research and Competitor Analysis:**\n",
    "   - Businesses use web scraping to monitor competitors, track market trends, and gather information about products and pricing. Analyzing data from various websites can provide insights into market dynamics, consumer behavior, and industry trends.\n",
    "\n",
    "4. **Content Aggregation:**\n",
    "   - Web scraping is often used to aggregate content from different websites. News aggregators, job boards, and real estate platforms may use web scraping to collect and display relevant information from multiple sources in one place.\n",
    "\n",
    "5. **Sentiment Analysis:**\n",
    "   - Social media platforms, forums, and review sites contain valuable user-generated content. Web scraping can be employed to gather and analyze this content, helping businesses understand public sentiment, customer feedback, and trends related to their products or services.\n",
    "\n",
    "### Three Areas Where Web Scraping is Used:\n",
    "\n",
    "1. **E-commerce Price Monitoring:**\n",
    "   - Web scraping is commonly used in e-commerce for monitoring and tracking product prices across different online stores. Retailers and consumers use this data for price comparison, and businesses use it for competitive pricing strategies.\n",
    "\n",
    "2. **Job Market Data Collection:**\n",
    "   - Job search platforms and recruiters use web scraping to collect and aggregate job postings from various websites. This allows them to provide comprehensive job search services, analyze market demand, and track employment trends.\n",
    "\n",
    "3. **Financial Data Extraction:**\n",
    "   - Financial analysts and investors use web scraping to extract financial data, stock prices, and economic indicators from various financial websites. This information is crucial for making investment decisions, conducting market research, and analyzing economic trends.\n",
    "\n",
    "Web scraping is a powerful tool, but it should be used responsibly and ethically. It's important to be aware of and comply with the terms of service of the websites being scraped, and to avoid overloading servers with too many requests. Additionally, some websites may have legal restrictions on data scraping, so it's essential to understand and respect those limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b14e9f-2016-41f5-ad87-160df1a7967b",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f86cf3-2f7b-4c7d-aa7a-293a043ca846",
   "metadata": {},
   "source": [
    "Web scraping can be performed using various methods and tools, each with its own advantages and use cases. Here are some common methods used for web scraping:\n",
    "\n",
    "Manual Copy-Pasting:\n",
    "\n",
    "The simplest form of web scraping involves manually copying and pasting information from a website into a local file or spreadsheet. While straightforward, this method is time-consuming and not suitable for large-scale data extraction.\n",
    "Regular Expressions (Regex):\n",
    "\n",
    "Regular expressions can be used to extract specific patterns of text from HTML content. While powerful, regex can be complex and brittle, especially when dealing with the hierarchical and nested structure of HTML. It is generally not recommended for parsing HTML due to its limitations in handling complex document structures.\n",
    "HTML Parsing with Libraries:\n",
    "\n",
    "Many programming languages have libraries for parsing HTML, such as BeautifulSoup for Python and jsoup for Java. These libraries provide a convenient way to navigate the HTML structure, extract data, and handle complex document hierarchies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3a503f-fd27-45cd-b881-72f0564bc0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using BeautifulSoup in Python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extracting text content from a specific HTML tag\n",
    "title = soup.find('title').text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063993b-a873-4d7b-b1b0-77f3d3023dae",
   "metadata": {},
   "source": [
    "Web Scraping Frameworks:\n",
    "\n",
    "There are dedicated web scraping frameworks, such as Scrapy (Python) and Puppeteer (JavaScript). These frameworks provide higher-level abstractions for building web scrapers, handling asynchronous requests, and managing the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631f0576-890b-4d99-a2fd-dbb8badec6a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scrapy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example using Scrapy in Python\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMySpider\u001b[39;00m(scrapy\u001b[38;5;241m.\u001b[39mSpider):\n\u001b[1;32m      5\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_spider\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scrapy'"
     ]
    }
   ],
   "source": [
    "# Example using Scrapy in Python\n",
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'my_spider'\n",
    "    start_urls = ['https://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').get()\n",
    "        yield {'title': title}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846af0a-6530-4de2-af84-b700c7216195",
   "metadata": {},
   "source": [
    "Headless Browsers:\n",
    "\n",
    "Headless browsers, like Selenium, automate the interaction with websites by simulating user behavior. They can be used for dynamic web pages that rely on JavaScript to load content. Selenium allows you to programmatically control a browser, navigate through pages, and extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "684428d1-7a67-4225-8b53-4e2379e7b29d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example using Selenium in Python\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://example.com\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "# Example using Selenium in Python\n",
    "from selenium import webdriver\n",
    "\n",
    "url = 'https://example.com'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "# Extracting text content using Selenium\n",
    "title = driver.find_element_by_tag_name('title').text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1a4a8-b2b9-47a4-a0ef-f3715cc4a06b",
   "metadata": {},
   "source": [
    "APIs (Application Programming Interfaces):\n",
    "\n",
    "Some websites provide APIs that allow developers to access data in a structured and organized manner. While not traditional web scraping, using APIs is a more reliable and sanctioned way to obtain data from websites.\n",
    "Web scraping methods vary in complexity, and the choice of method depends on factors such as the structure of the website, the amount of data to be scraped, and the desired level of automation. It's important to be aware of legal and ethical considerations when scraping data from websites and to respect the terms of service of the target sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02333bf8-b4bd-4c19-92d3-2a659b1f47f4",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dffe8d1-3d9f-479b-9951-b9350798fb8e",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library used for web scraping purposes to pull the data out of HTML and XML files. It provides Pythonic idioms for iterating, searching, and modifying the parse tree, making it easy to extract relevant information from web pages. Beautiful Soup creates a parse tree from a page source code that can be used to extract data in a hierarchical and more readable manner.\n",
    "\n",
    "Key features of Beautiful Soup:\n",
    "HTML and XML Parsing:\n",
    "\n",
    "Beautiful Soup provides a convenient way to parse HTML and XML documents. It converts the raw HTML/XML content into a parse tree, allowing users to navigate and search the document easily.\n",
    "Traversal:\n",
    "\n",
    "Beautiful Soup allows users to navigate the parse tree using methods like find(), find_all(), select(), and more. These methods enable the extraction of specific tags, attributes, or text content from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac79bf8-7085-4d6f-9bd0-a0780ac94245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of finding a specific tag\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = '<p>This is a <b>paragraph</b>.</p>'\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extracting the text content of the paragraph\n",
    "paragraph_text = soup.find('p').text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d5421-0a72-4184-bef1-cdccbbfa079c",
   "metadata": {},
   "source": [
    "Tag and Attribute Handling:\n",
    "\n",
    "Beautiful Soup provides methods to work with HTML tags and their attributes. Users can access tag names, attributes, modify values, and navigate the document based on these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0cb84d7-ab2b-4691-853c-07a15b484041",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tag \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m tag_name \u001b[38;5;241m=\u001b[39m tag\u001b[38;5;241m.\u001b[39mname  \u001b[38;5;66;03m# 'b'\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m attribute_value \u001b[38;5;241m=\u001b[39m \u001b[43mtag\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Accessing attribute value\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bs4/element.py:1519\u001b[0m, in \u001b[0;36mTag.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;124;03m\"\"\"tag[key] returns the value of the 'key' attribute for the Tag,\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;124;03m    and throws an exception if it's not there.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "# Example of accessing tag attributes\n",
    "tag = soup.find('b')\n",
    "tag_name = tag.name  # 'b'\n",
    "attribute_value = tag['class']  # Accessing attribute value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a1994-09ed-4ae0-a7b8-c26d2df5b19f",
   "metadata": {},
   "source": [
    "Searching and Filtering:\n",
    "\n",
    "Beautiful Soup allows users to search for specific tags or filter elements based on various criteria, such as class, id, attributes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cff8a7-8ad7-41ce-97a9-58901939d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of searching and filtering\n",
    "paragraphs_with_class = soup.find_all('p', class_='highlight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c68cc-6f92-434f-b2b0-018651f5ce00",
   "metadata": {},
   "source": [
    "Modifying and Creating Documents:\n",
    "\n",
    "Beautiful Soup supports the modification and creation of HTML/XML documents. Users can add, modify, or remove tags and their attributes, making it versatile for both parsing and generating content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb9d8012-116f-43e4-93a5-df93a493c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of modifying content\n",
    "tag = soup.find('p')\n",
    "tag.string = 'New paragraph content'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee24c3-3603-4e94-9b29-22aea4ee32ac",
   "metadata": {},
   "source": [
    "Why is Beautiful Soup Used?\n",
    "Ease of Use:\n",
    "\n",
    "Beautiful Soup provides a simple and Pythonic way to navigate, search, and manipulate the parse tree. Its syntax is designed to be intuitive and easy for users to work with.\n",
    "Robust HTML Parsing:\n",
    "\n",
    "Beautiful Soup is designed to handle imperfect HTML and XML documents. It can often parse and extract data from poorly formatted or messy HTML, making it a valuable tool for web scraping tasks.\n",
    "Compatibility:\n",
    "\n",
    "Beautiful Soup works well with popular Python parsers, such as the built-in html.parser, as well as external parsers like lxml and html5lib. This allows users to choose the parser that best fits their needs.\n",
    "Community Support:\n",
    "\n",
    "Beautiful Soup has a large and active community of users. This means there are plenty of resources, tutorials, and community support available for those who are learning or using the library.\n",
    "Integration with Requests:\n",
    "\n",
    "Beautiful Soup is often used in conjunction with the requests library for making HTTP requests and retrieving HTML content. This combination is commonly used for web scraping tasks.\n",
    "Beautiful Soup is a widely used library in the Python ecosystem for web scraping due to its flexibility, simplicity, and robust parsing capabilities. It allows developers to quickly extract relevant information from HTML and XML documents, making it a valuable tool for data extraction and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26443a55-7404-48c9-8a02-a6ad67554cbc",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e948c85-be35-474c-9ca2-bdc4a1ef4b4d",
   "metadata": {},
   "source": [
    "Flask is often used in web scraping projects for several reasons:\n",
    "\n",
    "1. **Web Application Development:**\n",
    "   - Flask is a lightweight web framework for Python that is well-suited for developing web applications. In a web scraping project, you may want to create a user interface to input parameters, display results, or control the scraping process. Flask makes it easy to build such web applications with minimal boilerplate code.\n",
    "\n",
    "2. **RESTful API Development:**\n",
    "   - Flask allows you to create RESTful APIs effortlessly. This is beneficial if you want to expose the web scraping functionality as an API, making it accessible to other applications or services. You can create endpoints for initiating scraping tasks, retrieving data, or interacting with the scraping process.\n",
    "\n",
    "3. **HTML Rendering and Parsing:**\n",
    "   - When scraping dynamic websites that heavily rely on JavaScript to load content, you may need a headless browser or a tool like Selenium to render and interact with the HTML. Flask can be used to create a simple web server for hosting a web page that uses JavaScript to trigger the scraping process, making it easier to handle dynamic content.\n",
    "\n",
    "4. **Data Visualization:**\n",
    "   - Flask integrates well with various front-end libraries and frameworks for data visualization, such as D3.js, Plotly, or Chart.js. If your web scraping project involves analyzing and presenting scraped data in graphical form, Flask can be used to serve the visualizations to users.\n",
    "\n",
    "5. **User Authentication and Authorization:**\n",
    "   - Flask provides tools for implementing user authentication and authorization. If your web scraping project requires user accounts, access control, or personalized experiences, Flask can handle user management and session handling.\n",
    "\n",
    "6. **Structured Project Organization:**\n",
    "   - Flask follows a modular structure, making it easy to organize your web scraping project into different components such as routes, templates, and static files. This promotes clean code organization and maintainability.\n",
    "\n",
    "7. **Community and Documentation:**\n",
    "   - Flask has a large and active community, and it is well-documented. If you encounter challenges or need assistance with your web scraping project, there are ample resources available, including tutorials, forums, and community discussions related to Flask.\n",
    "\n",
    "8. **Rapid Prototyping:**\n",
    "   - Flask is known for its simplicity and ease of use. This makes it an excellent choice for rapidly prototyping and developing the initial version of your web scraping project. You can quickly set up routes, create HTML templates, and test your scraping logic.\n",
    "\n",
    "While Flask is not strictly required for every web scraping project, its features make it a convenient and versatile choice, especially when there is a need for web application development, API creation, or integrating web scraping functionality with a user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c85d4f-235d-42d5-86a9-1158ce007edc",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc9fbb-88a1-4cf2-a41b-913cffcf92f0",
   "metadata": {},
   "source": [
    "The specific AWS services used in a web scraping project can vary based on the project requirements, architecture, and design choices. However, here are some AWS services that might be relevant in the context of a web scraping project:\n",
    "\n",
    "1. **Amazon EC2 (Elastic Compute Cloud):**\n",
    "   - **Use:** EC2 instances are virtual servers in the cloud, and they can be used to host the web scraping application. The application logic, web server, and any associated scripts or processes can run on EC2 instances.\n",
    "\n",
    "2. **Amazon S3 (Simple Storage Service):**\n",
    "   - **Use:** S3 is a scalable object storage service. In a web scraping project, S3 can be used to store and manage large volumes of scraped data, logs, or any other files generated during the scraping process. This provides a reliable and scalable storage solution.\n",
    "\n",
    "3. **Amazon RDS (Relational Database Service):**\n",
    "   - **Use:** RDS provides managed relational databases in the cloud. If your web scraping project involves storing structured data in a relational database (e.g., metadata about scraped items), you can use RDS to host a database like MySQL, PostgreSQL, or others.\n",
    "\n",
    "4. **Amazon Lambda:**\n",
    "   - **Use:** Lambda allows you to run code without provisioning or managing servers. In the context of web scraping, Lambda functions can be used for specific tasks, such as preprocessing scraped data, triggering scraping tasks at scheduled intervals, or performing lightweight data processing.\n",
    "\n",
    "5. **Amazon API Gateway:**\n",
    "   - **Use:** API Gateway enables you to create and publish RESTful APIs. If your web scraping project includes exposing scraping functionalities via an API, API Gateway can be used to manage and expose these APIs securely.\n",
    "\n",
    "6. **Amazon CloudWatch:**\n",
    "   - **Use:** CloudWatch is a monitoring and logging service. In a web scraping project, CloudWatch can be used to collect and track logs, set up alarms for specific events, and gain insights into the performance and health of your application.\n",
    "\n",
    "7. **Amazon DynamoDB:**\n",
    "   - **Use:** DynamoDB is a fully managed NoSQL database service. If your web scraping project involves handling unstructured or semi-structured data and you prefer a NoSQL database, DynamoDB can be used for efficient and scalable storage.\n",
    "\n",
    "8. **Amazon SQS (Simple Queue Service):**\n",
    "   - **Use:** SQS is a fully managed message queuing service. In a web scraping project, SQS can be used to decouple and manage tasks or messages between different components of your architecture, providing a scalable and reliable messaging system.\n",
    "\n",
    "9. **Amazon SNS (Simple Notification Service):**\n",
    "   - **Use:** SNS is a fully managed messaging service for coordinating the sending of messages to distributed systems. If your web scraping project requires notifications or alerts based on specific events (e.g., completion of a scraping task), SNS can be used to send messages to various endpoints.\n",
    "\n",
    "10. **AWS Glue:**\n",
    "    - **Use:** AWS Glue is a fully managed extract, transform, and load (ETL) service. If your web scraping project involves transforming or cleaning data before storage or analysis, Glue can be used to automate ETL jobs.\n",
    "\n",
    "These are just examples, and the choice of AWS services will depend on the specific requirements and architecture of your web scraping project. It's essential to consider factors such as data volume, scalability, performance, and security when selecting AWS services for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35ed440-3d77-4aa2-b34e-58a8f766f23a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
