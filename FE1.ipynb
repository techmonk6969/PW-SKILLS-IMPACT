{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26dbcbf3-d750-448f-895d-5314c8e90309",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e58cd-28bd-4534-9bcc-2f653571f88f",
   "metadata": {},
   "source": [
    "**Missing Values in a Dataset:**\n",
    "\n",
    "**Definition:**\n",
    "- Missing values in a dataset refer to the absence of data for particular observations or features. It is represented as NaN (Not a Number), null, or any other designated placeholder.\n",
    "\n",
    "**Importance of Handling Missing Values:**\n",
    "\n",
    "1. **Impact on Analysis:**\n",
    "   - Missing values can lead to biased or inaccurate analyses, affecting the validity of statistical inferences and machine learning models.\n",
    "\n",
    "2. **Model Performance:**\n",
    "   - Many machine learning algorithms cannot handle missing values directly and may fail or provide suboptimal performance.\n",
    "\n",
    "3. **Data Integrity:**\n",
    "   - Imputing or handling missing values ensures the completeness and integrity of the dataset, preventing gaps in information.\n",
    "\n",
    "4. **Biased Results:**\n",
    "   - Ignoring missing values can lead to biased results, as the available data may not be representative of the entire population.\n",
    "\n",
    "5. **Informed Decision-Making:**\n",
    "   - Handling missing values enables researchers and practitioners to make informed decisions based on the entire dataset.\n",
    "\n",
    "**Algorithms Not Affected by Missing Values:**\n",
    "\n",
    "While many machine learning algorithms require complete datasets, some are designed to handle missing values inherently:\n",
    "\n",
    "1. **Tree-Based Models:**\n",
    "   - Decision trees, Random Forest, and Gradient Boosted Trees can naturally handle missing values without explicit imputation.\n",
    "\n",
    "2. **K-Nearest Neighbors (KNN):**\n",
    "   - KNN can work with missing values by considering only non-missing features when determining the similarity between data points.\n",
    "\n",
    "3. **Naive Bayes:**\n",
    "   - Naive Bayes is generally not affected by missing values. It calculates probabilities independently for each feature.\n",
    "\n",
    "4. **Support Vector Machines (SVM):**\n",
    "   - SVMs can handle missing values indirectly, as the decision boundaries are based on support vectors rather than the entire dataset.\n",
    "\n",
    "5. **Deep Learning (Neural Networks):**\n",
    "   - Some deep learning architectures, especially neural networks with certain types of layers (e.g., embedding layers), can handle missing values.\n",
    "\n",
    "6. **Robust Regression Models:**\n",
    "   - Robust regression techniques, such as Huber regression, can be less sensitive to outliers and missing values.\n",
    "\n",
    "**Handling Missing Values:**\n",
    "\n",
    "1. **Imputation:**\n",
    "   - Fill missing values with estimated values (e.g., mean, median, mode, regression imputation).\n",
    "\n",
    "2. **Deletion:**\n",
    "   - Remove rows or columns with missing values, but this should be done cautiously to avoid significant data loss.\n",
    "\n",
    "3. **Advanced Techniques:**\n",
    "   - Use more advanced methods like k-Nearest Neighbors imputation, matrix factorization, or deep learning approaches for imputing missing values.\n",
    "\n",
    "4. **Indicator Variables:**\n",
    "   - Create indicator variables to denote the presence of missing values, allowing models to consider them as a separate category.\n",
    "\n",
    "Handling missing values is a critical step in the data preprocessing pipeline, ensuring accurate analyses and reliable model performance. The choice of method depends on the nature of the dataset, the extent of missingness, and the characteristics of the machine learning algorithm being employed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65965c4a-296f-4f30-9604-ea345e5705cc",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a01daa5-aed4-4ac2-8872-f27c167a899b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  NaN\n",
      "2  NaN  7.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame after dropping rows with missing values:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame after dropping columns with missing values:\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_dropped_rows = df.dropna()\n",
    "\n",
    "# Drop columns with any missing values\n",
    "df_dropped_columns = df.dropna(axis=1)\n",
    "\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "print(\"\\nDataFrame after dropping rows with missing values:\\n\", df_dropped_rows)\n",
    "print(\"\\nDataFrame after dropping columns with missing values:\\n\", df_dropped_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f24cb093-ab74-450f-889f-3b2d1d95ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  NaN\n",
      "2  NaN  7.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame after mean imputation:\n",
      "           A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n",
      "\n",
      "DataFrame after median imputation:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  7.0\n",
      "2  2.0  7.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame after mode imputation:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  5.0\n",
      "2  1.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values with mean of respective columns\n",
    "df_imputed_mean = df.fillna(df.mean())\n",
    "\n",
    "# Impute missing values with median of respective columns\n",
    "df_imputed_median = df.fillna(df.median())\n",
    "\n",
    "# Impute missing values with mode of respective columns\n",
    "df_imputed_mode = df.fillna(df.mode().iloc[0])\n",
    "\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "print(\"\\nDataFrame after mean imputation:\\n\", df_imputed_mean)\n",
    "print(\"\\nDataFrame after median imputation:\\n\", df_imputed_median)\n",
    "print(\"\\nDataFrame after mode imputation:\\n\", df_imputed_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9404b8cd-c80a-4337-9af7-9e8f18e39ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  NaN\n",
      "2  NaN  7.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame after KNN imputation:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.5\n",
      "2  2.5  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Impute missing values using KNN\n",
    "df_imputed_knn = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "print(\"\\nDataFrame after KNN imputation:\\n\", df_imputed_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3530d05b-de40-44c3-bfeb-48533eb1c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  NaN\n",
      "2  NaN  7.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame after interpolation:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.0\n",
      "2  3.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Interpolate missing values\n",
    "df_interpolated = df.interpolate()\n",
    "\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "print(\"\\nDataFrame after interpolation:\\n\", df_interpolated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b650c-e151-46c2-b7ec-29c50e350ac7",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae785958-d494-454e-9f3d-a5b5693c2826",
   "metadata": {},
   "source": [
    "**Imbalanced Data:**\n",
    "\n",
    "Imbalanced data refers to a situation in a classification problem where the distribution of class labels is not equal; one class significantly outnumbers the other(s). For example, in binary classification, it occurs when one class has a much larger number of instances than the other class.\n",
    "\n",
    "**Consequences of Imbalanced Data:**\n",
    "1. **Bias Towards the Majority Class:**\n",
    "   - Classifiers tend to be biased towards the majority class because they are designed to maximize overall accuracy. As a result, they may struggle to correctly predict instances of the minority class.\n",
    "\n",
    "2. **Poor Generalization:**\n",
    "   - The model may not generalize well to the minority class, leading to poor performance on real-world data where both classes are important.\n",
    "\n",
    "3. **Misleading Evaluation Metrics:**\n",
    "   - Common classification metrics like accuracy can be misleading in imbalanced datasets. A high accuracy may result from correctly predicting the majority class, while the minority class is often misclassified.\n",
    "\n",
    "4. **Model Skewing:**\n",
    "   - The model may learn patterns that are specific to the majority class and ignore the minority class. This can lead to a skewed and biased model.\n",
    "\n",
    "**Impact of Not Handling Imbalanced Data:**\n",
    "1. **Poor Predictive Performance:**\n",
    "   - The model's ability to predict the minority class is compromised, affecting its overall predictive performance.\n",
    "\n",
    "2. **Misleading Confidence:**\n",
    "   - The model may assign high confidence to incorrect predictions, especially for the majority class, giving a false sense of reliability.\n",
    "\n",
    "3. **Uninformed Decision-Making:**\n",
    "   - In applications where correct predictions for the minority class are crucial (e.g., fraud detection, rare diseases), the unaddressed imbalance may lead to uninformed decision-making.\n",
    "\n",
    "4. **Model Fairness Issues:**\n",
    "   - Imbalanced data can introduce fairness issues, especially when the minority class represents a group that deserves equal consideration.\n",
    "\n",
    "**Handling Imbalanced Data:**\n",
    "1. **Resampling Techniques:**\n",
    "   - Oversampling the minority class or undersampling the majority class to balance the class distribution.\n",
    "\n",
    "2. **Synthetic Data Generation:**\n",
    "   - Techniques like SMOTE (Synthetic Minority Over-sampling Technique) generate synthetic instances of the minority class.\n",
    "\n",
    "3. **Cost-Sensitive Learning:**\n",
    "   - Assigning different misclassification costs to different classes, emphasizing the importance of correct predictions for the minority class.\n",
    "\n",
    "4. **Ensemble Methods:**\n",
    "   - Using ensemble methods like Random Forests or Gradient Boosting, which can handle imbalanced data more effectively.\n",
    "\n",
    "5. **Different Evaluation Metrics:**\n",
    "   - Using metrics such as precision, recall, F1-score, or area under the ROC curve (AUC-ROC) that are more sensitive to the performance on the minority class.\n",
    "\n",
    "Addressing imbalanced data is crucial for building models that generalize well and make informed predictions, especially in scenarios where the minority class is of significant interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389adc2c-3c6b-40a8-ac50-d206c0e028f8",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ff6ac-af91-4c3e-817f-7bbadef0a998",
   "metadata": {},
   "source": [
    "\n",
    "Up-sampling and Down-sampling:\n",
    "\n",
    "1. Up-sampling:\n",
    "\n",
    "Definition: Up-sampling involves increasing the number of instances in the minority class to balance the class distribution.\n",
    "Example Scenario:\n",
    "In a credit card fraud detection system, where fraudulent transactions are rare (minority class), up-sampling might be necessary to ensure the model can learn meaningful patterns associated with fraud.\n",
    "2. Down-sampling:\n",
    "\n",
    "Definition: Down-sampling involves decreasing the number of instances in the majority class to balance the class distribution.\n",
    "Example Scenario:\n",
    "In a medical diagnosis system where the majority of patients don't have a rare disease, down-sampling may be applied to create a more balanced dataset and prevent the model from being biased towards predicting the majority class.\n",
    "When Up-sampling and Down-sampling are Required:\n",
    "\n",
    "Up-sampling (Increasing Minority Class Instances):\n",
    "\n",
    "Scenario: When the minority class is underrepresented, and the model struggles to capture its patterns due to insufficient instances.\n",
    "Example:\n",
    "Consider a dataset where 95% of emails are non-spam (majority class) and 5% are spam (minority class). To build an effective spam classifier, up-sampling the spam instances may be necessary to avoid bias towards non-spam.\n",
    "Down-sampling (Decreasing Majority Class Instances):\n",
    "\n",
    "Scenario: When the majority class overwhelms the dataset, leading to biased predictions and poor generalization to the minority class.\n",
    "Example:\n",
    "In a manufacturing process where 98% of products pass quality control (majority class) and 2% are defective (minority class), down-sampling may be required to prevent the model from simply predicting all products as non-defective.\n",
    "Methods for Up-sampling and Down-sampling:\n",
    "\n",
    "Up-sampling:\n",
    "\n",
    "Random Over-sampling: Duplicating random instances of the minority class.\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): Creating synthetic instances to expand the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b756dc-2c30-447d-8819-ab3deaa27eef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m      2\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m SMOTE()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0be06f-6985-483c-9013-cbb5faebf54f",
   "metadata": {},
   "source": [
    "Down-sampling:\n",
    "\n",
    "Random Under-sampling: Randomly removing instances from the majority class.\n",
    "Tomek Links: Removing pairs of instances (one from the minority and one from the majority class) that are close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9c667-db87-4d51-b81b-3741172d2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_resampled, y_resampled = RandomUnderSampler().fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1110fd-62fe-40bd-919f-88fb4c422fe4",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94648b-158b-49f0-a6c4-c7d79f8ca847",
   "metadata": {},
   "source": [
    "\n",
    "Data Augmentation:\n",
    "\n",
    "Definition: Data augmentation is a technique used to artificially increase the size of a dataset by applying various transformations to the existing data, creating new samples. This is commonly employed in machine learning, especially in computer vision tasks, to enhance model generalization by exposing it to diverse variations of the input data.\n",
    "\n",
    "Methods of Data Augmentation:\n",
    "\n",
    "Image Rotation: Rotating images at different angles.\n",
    "Flipping: Mirroring images horizontally or vertically.\n",
    "Zooming: Zooming in or out of images.\n",
    "Translation: Shifting images horizontally or vertically.\n",
    "Changing Brightness and Contrast: Adjusting the brightness and contrast of images.\n",
    "Adding Noise: Introducing random noise to the images.\n",
    "SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "Definition: SMOTE is a specific data augmentation technique designed for handling imbalanced datasets, particularly in classification tasks where the minority class is underrepresented.\n",
    "\n",
    "How SMOTE Works:\n",
    "\n",
    "Synthetic Sample Generation: SMOTE generates synthetic samples for the minority class by interpolating between existing minority class instances.\n",
    "Interpolation: For each minority class instance, SMOTE selects its k nearest neighbors.\n",
    "Feature Space Interpolation: A synthetic sample is created by linearly interpolating features of the selected instance and its neighbors.\n",
    "Randomness: Randomness is introduced to the interpolation process to generate diverse synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21e865-a2ac-4f7a-aeda-66b832771ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c97f79-5087-4c7f-af0f-64f444c052d0",
   "metadata": {},
   "source": [
    "Advantages of SMOTE:\n",
    "\n",
    "Addresses class imbalance by creating synthetic instances of the minority class.\n",
    "Reduces the risk of the model being biased towards the majority class.\n",
    "Considerations:\n",
    "\n",
    "While SMOTE is effective in improving model performance on imbalanced datasets, it may not always be suitable for all scenarios.\n",
    "The choice of the k parameter (number of nearest neighbors) and the application context should be considered to avoid introducing noise or oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa07f61c-d21c-4375-8b87-2287ac679937",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325c19e-8fdb-4b97-acf8-8a01e09e52a5",
   "metadata": {},
   "source": [
    "Outliers in a Dataset:\n",
    "\n",
    "Definition: Outliers are data points that significantly differ from the rest of the observations in a dataset. These observations are unusually high or low in value compared to the majority of the data.\n",
    "\n",
    "Characteristics of Outliers:\n",
    "\n",
    "Unusual Values: Outliers deviate significantly from the typical pattern in the dataset.\n",
    "Impact on Statistics: They can heavily influence summary statistics such as the mean and standard deviation.\n",
    "Potential Errors: Outliers might be indicative of errors in data collection or measurement.\n",
    "Importance of Handling Outliers:\n",
    "\n",
    "Distorted Statistics: Outliers can distort statistical measures, leading to inaccurate insights about the central tendency and spread of the data.\n",
    "Model Performance: Outliers can negatively impact the performance of machine learning models, especially those sensitive to variations in data.\n",
    "Assumption Violations: Some statistical techniques assume a normal distribution, and the presence of outliers can violate these assumptions.\n",
    "Data Quality: Handling outliers improves the overall quality and reliability of the dataset.\n",
    "Methods for Handling Outliers:\n",
    "\n",
    "Removing Outliers: Exclude extreme values from the dataset.\n",
    "Transformations: Apply mathematical transformations to reduce the impact of outliers (e.g., log transformation).\n",
    "Imputation: Replace outliers with values derived from the rest of the data.\n",
    "Binning: Grouping values into bins to mitigate the impact of extreme values.\n",
    "Model-based Approaches: Use robust models less sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0cda54-cf62-4be0-a1a0-07c06f3540ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'column_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'column_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assume df is the DataFrame\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m Q1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolumn_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.25\u001b[39m)\n\u001b[1;32m      5\u001b[0m Q3 \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.75\u001b[39m)\n\u001b[1;32m      6\u001b[0m IQR \u001b[38;5;241m=\u001b[39m Q3 \u001b[38;5;241m-\u001b[39m Q1\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'column_name'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume df is the DataFrame\n",
    "Q1 = df['column_name'].quantile(0.25)\n",
    "Q3 = df['column_name'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Removing outliers\n",
    "df_no_outliers = df[(df['column_name'] >= Q1 - 1.5 * IQR) & (df['column_name'] <= Q3 + 1.5 * IQR)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fb708-3a68-4892-8d44-efb5d140740a",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e197e-b8d1-45c5-a62c-18be9b84c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_missing = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b4bc2-d945-4f59-9586-df1952a8e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value = df['column_name'].mean()\n",
    "df['column_name'].fillna(mean_value, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c7cab-a695-4cb0-bfad-a7a8e179397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = knn_imputer.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c8d5fb-4bfa-45f2-8743-4b665eee7b75",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84aabfa-542b-4824-a4e0-3962e1b7b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92decde5-ff69-487d-9bea-3a950a1ac1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(df.isnull(), cbar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ebea9-9c93-4d79-8f00-a373bbd98d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['column_with_missing'], df['other_variable'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ef51c-d089-4a7a-a1a0-da7316514f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['variable1', 'variable2']].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876e823-f77f-4f3c-9f3c-76a30a5598fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from missingpy import MissForest\n",
    "imputer = MissForest()\n",
    "df_imputed = imputer.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff890d20-b94e-474f-9cdd-782f58382ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc72e1-fc30-4963-bc35-b00571108866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = df.fillna(df.mean())  # Example imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f38d18-6667-4d7b-ae05-83993cbd2bbe",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab35cfba-cc24-4a4b-b387-0145ad82fcb9",
   "metadata": {},
   "source": [
    "Dealing with imbalanced datasets, where the distribution of classes is uneven, is common in various machine learning applications, including medical diagnosis. Here are some strategies to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "\n",
    "1. **Use Appropriate Metrics:**\n",
    "   - Instead of accuracy, which might be misleading in imbalanced datasets, use metrics that provide a more comprehensive view:\n",
    "     - **Precision:** Focus on the proportion of correctly predicted positive instances among all predicted positives.\n",
    "     - **Recall (Sensitivity):** Emphasize the ability to capture all actual positive instances.\n",
    "     - **F1-Score:** Harmonic mean of precision and recall.\n",
    "\n",
    "2. **Confusion Matrix Analysis:**\n",
    "   - Examine the confusion matrix to understand how well the model is performing in terms of true positives, false positives, true negatives, and false negatives.\n",
    "   ```python\n",
    "   from sklearn.metrics import confusion_matrix\n",
    "   conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "   ```\n",
    "\n",
    "3. **ROC Curve and AUC-ROC Score:**\n",
    "   - Plotting the Receiver Operating Characteristic (ROC) curve and calculating the Area Under the Curve (AUC) provides insights into the trade-off between true positive rate and false positive rate.\n",
    "   ```python\n",
    "   from sklearn.metrics import roc_curve, auc\n",
    "   fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "   auc_score = auc(fpr, tpr)\n",
    "   ```\n",
    "\n",
    "4. **Precision-Recall Curve and AUC-PR Score:**\n",
    "   - Evaluate the precision-recall curve and AUC-PR score to understand the model's performance across different probability thresholds.\n",
    "   ```python\n",
    "   from sklearn.metrics import precision_recall_curve, auc\n",
    "   precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "   auc_pr_score = auc(recall, precision)\n",
    "   ```\n",
    "\n",
    "5. **Stratified Sampling and Cross-Validation:**\n",
    "   - Use techniques like stratified sampling and cross-validation to ensure that each fold or batch maintains the original class distribution.\n",
    "\n",
    "6. **Cost-sensitive Learning:**\n",
    "   - Assign different misclassification costs to different classes to reflect the real-world consequences of misclassifying rare positive instances.\n",
    "\n",
    "7. **Ensemble Methods:**\n",
    "   - Explore ensemble methods like Random Forests or Gradient Boosting, which are often more robust to imbalanced datasets.\n",
    "\n",
    "8. **Class Balancing Techniques:**\n",
    "   - Implement various class balancing techniques such as oversampling the minority class (e.g., SMOTE) or undersampling the majority class.\n",
    "\n",
    "9. **Adjust Decision Threshold:**\n",
    "   - Adjust the decision threshold of the model to balance precision and recall based on the specific requirements of the problem.\n",
    "\n",
    "10. **Domain Expert Consultation:**\n",
    "    - Seek input from domain experts to understand the criticality of false positives and false negatives in the context of the application.\n",
    "\n",
    "By employing these strategies, you can gain a more nuanced understanding of your model's performance and make informed decisions regarding its deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f16acb-85d7-48d9-8485-2732135fe8f5",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3649be-ece6-4315-8666-f2c387138eb6",
   "metadata": {},
   "source": [
    "When dealing with an unbalanced dataset, particularly in the context of estimating customer satisfaction where the majority class dominates, you may want to employ down-sampling techniques to balance the classes. Here are some methods to down-sample the majority class:\n",
    "\n",
    "1. **Random Under-Sampling:**\n",
    "   - Randomly remove instances from the majority class until a balanced distribution is achieved.\n",
    "   ```python\n",
    "   from sklearn.utils import resample\n",
    "\n",
    "   # Assuming X_train and y_train are your feature and target variables\n",
    "   X_train_resampled, y_train_resampled = resample(X_train[y_train == majority_class], \n",
    "                                                  y_train[y_train == majority_class],\n",
    "                                                  replace=False,\n",
    "                                                  n_samples=len(y_train[y_train == minority_class]),\n",
    "                                                  random_state=42)\n",
    "\n",
    "   X_balanced = np.concatenate((X_train[y_train == minority_class], X_train_resampled))\n",
    "   y_balanced = np.concatenate((y_train[y_train == minority_class], y_train_resampled))\n",
    "   ```\n",
    "\n",
    "2. **Tomek Links:**\n",
    "   - Remove instances from the majority class that are Tomek links with instances from the minority class. Tomek links are pairs of instances (one from each class) that are closest to each other.\n",
    "   ```python\n",
    "   from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "   tl = TomekLinks()\n",
    "   X_balanced, y_balanced = tl.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "3. **NearMiss:**\n",
    "   - NearMiss is an under-sampling technique that selects instances from the majority class based on their distance to instances from the minority class.\n",
    "   ```python\n",
    "   from imblearn.under_sampling import NearMiss\n",
    "\n",
    "   nm = NearMiss()\n",
    "   X_balanced, y_balanced = nm.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "4. **Cluster Centroids:**\n",
    "   - Generate centroids based on clustering algorithms and remove instances from the majority class that are farthest from these centroids.\n",
    "   ```python\n",
    "   from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "   cc = ClusterCentroids(sampling_strategy='auto')\n",
    "   X_balanced, y_balanced = cc.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "5. **Edited Nearest Neighbors (ENN):**\n",
    "   - ENN removes instances from the majority class if their class label differs from the majority class label of their k-nearest neighbors.\n",
    "   ```python\n",
    "   from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "   enn = EditedNearestNeighbours()\n",
    "   X_balanced, y_balanced = enn.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "Choose the method that best fits the characteristics of your dataset and the requirements of your analysis. Additionally, consider evaluating the performance of your model on both the original and down-sampled datasets to determine the impact of the sampling strategy on predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b751ef9-81da-49aa-adbf-69012036c1bc",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46bf6a0-044d-47d5-a9e2-90d8d7a4bc52",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset with a low percentage of occurrences of a rare event, you may want to employ up-sampling techniques to balance the classes. Here are some methods to up-sample the minority class:\n",
    "\n",
    "1. **Random Over-Sampling:**\n",
    "   - Randomly duplicate instances from the minority class until a balanced distribution is achieved.\n",
    "   ```python\n",
    "   from sklearn.utils import resample\n",
    "\n",
    "   # Assuming X_train and y_train are your feature and target variables\n",
    "   X_train_resampled, y_train_resampled = resample(X_train[y_train == minority_class], \n",
    "                                                  y_train[y_train == minority_class],\n",
    "                                                  replace=True,\n",
    "                                                  n_samples=len(y_train[y_train == majority_class]),\n",
    "                                                  random_state=42)\n",
    "\n",
    "   X_balanced = np.concatenate((X_train[y_train == majority_class], X_train_resampled))\n",
    "   y_balanced = np.concatenate((y_train[y_train == majority_class], y_train_resampled))\n",
    "   ```\n",
    "\n",
    "2. **SMOTE (Synthetic Minority Over-sampling Technique):**\n",
    "   - Generate synthetic samples for the minority class by selecting k-nearest neighbors and creating new instances along the lines connecting these neighbors.\n",
    "   ```python\n",
    "   from imblearn.over_sampling import SMOTE\n",
    "\n",
    "   smote = SMOTE()\n",
    "   X_balanced, y_balanced = smote.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "3. **ADASYN (Adaptive Synthetic Sampling):**\n",
    "   - Similar to SMOTE, but it adjusts the weights of the samples to focus more on difficult-to-learn instances.\n",
    "   ```python\n",
    "   from imblearn.over_sampling import ADASYN\n",
    "\n",
    "   adasyn = ADASYN()\n",
    "   X_balanced, y_balanced = adasyn.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "4. **Random Synthetic Minority Over-sampling (RO-SMOTE):**\n",
    "   - A variation of SMOTE that generates synthetic samples between a randomly selected minority instance and its nearest neighbor from a different class.\n",
    "   ```python\n",
    "   from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "   ros = RandomOverSampler(sampling_strategy='minority')\n",
    "   X_balanced, y_balanced = ros.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "5. **Borderline-SMOTE:**\n",
    "   - Focuses on the instances near the decision boundary and generates synthetic samples only for those instances.\n",
    "   ```python\n",
    "   from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "   bsmote = BorderlineSMOTE()\n",
    "   X_balanced, y_balanced = bsmote.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "Select the method that best fits the characteristics of your dataset and the requirements of your analysis. Additionally, consider evaluating the performance of your model on both the original and up-sampled datasets to determine the impact of the sampling strategy on predictive performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
