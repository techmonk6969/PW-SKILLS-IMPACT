{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484082e4-7bd6-4340-9f17-a162046e9039",
   "metadata": {},
   "source": [
    "Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17129511-6753-41a2-8d21-5ece3315ffc1",
   "metadata": {},
   "source": [
    "The filter method in feature selection is a technique that involves evaluating the relevance of each feature independently of the machine learning model. This method relies on statistical measures to assign a score to each feature, and features are selected or removed based on these scores. The primary goal is to identify features that have a strong relationship with the target variable.\n",
    "\n",
    "Here's how the filter method generally works:\n",
    "\n",
    "1. **Compute a Statistical Measure for Each Feature:**\n",
    "   - Use statistical measures such as correlation, mutual information, chi-square, or other relevant metrics to quantify the relationship between each feature and the target variable.\n",
    "\n",
    "2. **Assign Scores to Features:**\n",
    "   - Each feature is assigned a score based on its statistical measure. Higher scores indicate a stronger relationship with the target variable.\n",
    "\n",
    "3. **Rank or Select Features:**\n",
    "   - Features are either ranked based on their scores, or a threshold is set to select the top-n features. Features with scores above the threshold or in the top-n are retained, while others are discarded.\n",
    "\n",
    "4. **Build a Model:**\n",
    "   - Use the selected features to train a machine learning model.\n",
    "\n",
    "Common statistical measures used in the filter method include:\n",
    "\n",
    "- **Correlation Coefficient:** Measures the linear relationship between two variables.\n",
    "- **Mutual Information:** Measures the amount of information one variable provides about another.\n",
    "- **Chi-Square Test:** Assesses the independence of two categorical variables.\n",
    "- **ANOVA F-statistic:** Measures the difference in means among groups.\n",
    "\n",
    "Advantages of the filter method include simplicity, speed, and model independence. However, it may not capture complex relationships between features, and the selected features are chosen without considering the interactions between them.\n",
    "\n",
    "Here's a brief example using the correlation coefficient in Python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Select features with correlation above a threshold (e.g., 0.2)\n",
    "selected_features = correlation_matrix[abs(correlation_matrix['sepal length (cm)']) > 0.2].index\n",
    "\n",
    "# Display selected features\n",
    "print(selected_features)\n",
    "```\n",
    "\n",
    "In this example, features with a correlation coefficient above 0.2 with the 'sepal length' feature are selected. Adjust the threshold and metric based on the characteristics of your dataset and the type of relationship you want to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7837c0-b24b-4fe6-ac4b-0eb43017bc6f",
   "metadata": {},
   "source": [
    "Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1233564-8e28-4298-9290-b2c4eeb6159b",
   "metadata": {},
   "source": [
    "The Wrapper method and the Filter method are both techniques for feature selection in machine learning, but they differ in their approaches and how they utilize a machine learning model during the selection process.\n",
    "\n",
    "**Wrapper Method:**\n",
    "1. **Search Space Exploration:**\n",
    "   - The Wrapper method evaluates different subsets of features by treating feature selection as a search problem.\n",
    "   - It explores various combinations of features and evaluates each subset's performance using a predictive model.\n",
    "  \n",
    "2. **Model-based Evaluation:**\n",
    "   - It uses the performance of a machine learning model as a criterion for selecting features.\n",
    "   - Features are selected or eliminated based on the model's performance on a specific learning task (e.g., classification accuracy, regression performance).\n",
    "\n",
    "3. **Iterative Process:**\n",
    "   - The Wrapper method is an iterative process where different subsets of features are used to train and evaluate the model.\n",
    "   - The model is trained and tested multiple times with different feature subsets.\n",
    "\n",
    "4. **Examples:**\n",
    "   - Recursive Feature Elimination (RFE) is a common wrapper method where features are recursively removed based on model performance.\n",
    "   - Forward Selection and Backward Elimination are other examples where features are added or removed iteratively.\n",
    "\n",
    "**Filter Method:**\n",
    "1. **Statistical Measures:**\n",
    "   - The Filter method evaluates features independently of the machine learning model.\n",
    "   - It uses statistical measures to assess the relevance of each feature, such as correlation, mutual information, or other metrics.\n",
    "\n",
    "2. **No Model Training:**\n",
    "   - Unlike the Wrapper method, the Filter method does not involve training a machine learning model during the feature selection process.\n",
    "   - Features are selected or removed based on their standalone statistical characteristics.\n",
    "\n",
    "3. **Computational Efficiency:**\n",
    "   - The Filter method is computationally efficient because it does not require training and evaluating a model multiple times.\n",
    "   - It is typically faster than the Wrapper method.\n",
    "\n",
    "4. **Examples:**\n",
    "   - Correlation coefficient, mutual information, chi-square test, and ANOVA F-statistic are common statistical measures used in the Filter method.\n",
    "\n",
    "**Comparison:**\n",
    "- The Wrapper method is computationally more expensive because it involves training and evaluating a model multiple times. It can capture complex relationships between features but may be prone to overfitting.\n",
    "- The Filter method is faster but may overlook interactions between features. It is model-independent and simpler to implement.\n",
    "\n",
    "In summary, the key distinction lies in whether the feature selection process involves training a machine learning model iteratively (Wrapper) or relies on statistical measures without model training (Filter). Each method has its advantages and limitations, and the choice depends on the characteristics of the dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a82ac1-4fe4-4e25-8343-97e370401905",
   "metadata": {},
   "source": [
    "Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd0b14-d41d-4a76-abcd-92c038ea2b57",
   "metadata": {},
   "source": [
    "Embedded feature selection methods integrate the feature selection process into the model training itself. These methods automatically select the most relevant features during the training phase. Here are some common techniques used in Embedded feature selection:\n",
    "\n",
    "1. **LASSO (Least Absolute Shrinkage and Selection Operator):**\n",
    "   - LASSO is a linear regression technique that adds a penalty term to the ordinary least squares (OLS) objective function.\n",
    "   - The penalty term encourages sparsity in the feature coefficients, effectively selecting a subset of features.\n",
    "\n",
    "2. **Ridge Regression:**\n",
    "   - Similar to LASSO, Ridge Regression introduces a regularization term to the OLS objective function.\n",
    "   - While LASSO promotes sparsity, Ridge Regression tends to shrink the coefficients of less important features toward zero.\n",
    "\n",
    "3. **Elastic Net:**\n",
    "   - Elastic Net is a combination of LASSO and Ridge Regression, incorporating both L1 and L2 regularization terms.\n",
    "   - It balances the feature selection capabilities of LASSO with the stabilizing effect of Ridge Regression.\n",
    "\n",
    "4. **Decision Trees (e.g., Random Forest, Gradient Boosting):**\n",
    "   - Decision trees inherently perform feature selection by choosing the most informative features at each split.\n",
    "   - Random Forest and Gradient Boosting algorithms can be used for feature importance ranking, where less important features are naturally assigned lower importance scores.\n",
    "\n",
    "5. **Regularized Linear Models (e.g., Logistic Regression with L1 regularization):**\n",
    "   - Regularized linear models, such as logistic regression with L1 regularization, encourage sparsity in the learned coefficients, leading to feature selection.\n",
    "\n",
    "6. **XGBoost Feature Importance:**\n",
    "   - XGBoost, a popular gradient boosting algorithm, provides a feature importance ranking based on the contribution of each feature to the model's performance.\n",
    "   - It allows identifying and selecting the most influential features.\n",
    "\n",
    "7. **Support Vector Machines (SVM) with L1 regularization:**\n",
    "   - SVM can be adapted to include L1 regularization, which promotes sparsity in the support vectors and, consequently, feature selection.\n",
    "\n",
    "8. **Neural Networks with Dropout:**\n",
    "   - In neural networks, dropout is a regularization technique where random neurons are excluded during training.\n",
    "   - Dropout can be viewed as a form of feature selection as it prevents the reliance on specific neurons, encouraging the network to learn more robust and generalizable features.\n",
    "\n",
    "These embedded feature selection techniques are integrated into the model training process, allowing the model to simultaneously learn from the data and select the most relevant features. The choice of method depends on the specific characteristics of the dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392ba99-ce08-4d7a-b593-4e5527df7355",
   "metadata": {},
   "source": [
    "Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca585522-f2f7-482f-9a02-8a79594fde0f",
   "metadata": {},
   "source": [
    "While the Filter method is a popular and simple approach for feature selection, it has some drawbacks. Here are some of the limitations associated with the Filter method:\n",
    "\n",
    "1. **Independence Assumption:**\n",
    "   - The Filter method typically evaluates the relevance of features independently of each other. It may overlook interactions or dependencies between features, which could be crucial for predictive modeling.\n",
    "\n",
    "2. **No Consideration of Model Performance:**\n",
    "   - Filter methods assess feature importance based on statistical measures (e.g., correlation, mutual information) without considering the actual performance of a predictive model. Features selected based on statistical criteria may not necessarily contribute to better model performance.\n",
    "\n",
    "3. **Sensitivity to Scaling:**\n",
    "   - Filter methods can be sensitive to the scale of features. The results may vary if the scales of different features are not standardized. Some measures, such as correlation coefficients, are influenced by the scale of the variables.\n",
    "\n",
    "4. **Limited to Univariate Analysis:**\n",
    "   - Filter methods typically consider only the relationship between individual features and the target variable. They might miss relevant features when the importance of a feature is context-dependent or when feature interactions are essential.\n",
    "\n",
    "5. **Fixed Thresholds:**\n",
    "   - The use of fixed statistical thresholds (e.g., correlation coefficient above a certain value) may not be universally applicable across different datasets. Optimal feature selection criteria may vary depending on the characteristics of the data.\n",
    "\n",
    "6. **Feature Redundancy:**\n",
    "   - Filter methods might select features that are correlated with the target variable but also highly correlated with each other. This redundancy can result in the inclusion of similar information, leading to inefficiencies.\n",
    "\n",
    "7. **Insensitive to Model Complexity:**\n",
    "   - Filter methods do not consider the complexity of the predictive model. They may select features that, when combined in a more complex model, could provide better predictive performance.\n",
    "\n",
    "8. **Limited Exploration of Feature Combinations:**\n",
    "   - Since the Filter method assesses features independently, it might miss combinations of features that collectively contribute to predictive power. Other methods, such as Wrapper methods, may be more suitable for exploring feature combinations.\n",
    "\n",
    "While the Filter method has its limitations, it can still be a valuable initial step in feature selection, providing a quick and computationally efficient way to reduce the dimensionality of the dataset. Researchers and practitioners often use a combination of filter, wrapper, and embedded methods for a more comprehensive feature selection approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed91f0a-b4ac-4caa-877a-dd3cdfb1407d",
   "metadata": {},
   "source": [
    "Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d5ce4-48dd-4b99-88a3-3783a43afad8",
   "metadata": {},
   "source": [
    "The choice between the Filter method and the Wrapper method for feature selection depends on the specific characteristics of the dataset, computational resources, and the goals of the analysis. Here are some situations where you might prefer using the Filter method over the Wrapper method:\n",
    "\n",
    "1. **Large Datasets:**\n",
    "   - When dealing with large datasets, the computational cost of using Wrapper methods (which involve training and evaluating models for different subsets of features) can be high. In such cases, the Filter method, which evaluates features independently, may be more computationally efficient.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA):**\n",
    "   - In the initial stages of data analysis or exploratory data analysis, the Filter method can provide quick insights into the relationships between individual features and the target variable. It allows for a rapid assessment of feature relevance without the need for extensive model training.\n",
    "\n",
    "3. **High-Dimensional Data:**\n",
    "   - In high-dimensional datasets where the number of features is much larger than the number of samples, the Wrapper method may face challenges due to overfitting or increased computational demands. The Filter method can serve as an initial step to reduce dimensionality before applying more complex methods.\n",
    "\n",
    "4. **Preprocessing or Pre-filtering:**\n",
    "   - As a preprocessing step, the Filter method can be used to pre-filter features based on certain criteria (e.g., correlation, statistical tests). This can help remove irrelevant or noisy features before applying more resource-intensive methods like Wrapper methods.\n",
    "\n",
    "5. **Understanding Feature Importance:**\n",
    "   - If the primary goal is to gain a better understanding of the relationships between individual features and the target variable rather than optimizing model performance, the Filter method can be more straightforward and interpretable.\n",
    "\n",
    "6. **Feature Ranking:**\n",
    "   - If the goal is to rank features based on their individual importance, the Filter method provides a ranking mechanism without the need to train multiple models. This ranking can guide further feature selection or inform feature engineering.\n",
    "\n",
    "7. **Data Preprocessing for Different Models:**\n",
    "   - When preparing data for multiple models that have different requirements or assumptions, the Filter method can be used to preprocess features uniformly across models. It ensures that each model receives a consistent set of relevant features.\n",
    "\n",
    "While the Filter method has its advantages in certain scenarios, it's important to note that the choice between Filter and Wrapper methods is not mutually exclusive. In practice, a combination of both methods, along with careful consideration of the specific problem and dataset characteristics, often leads to more effective feature selection strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95162423-8c4f-423f-9454-b785a4afed58",
   "metadata": {},
   "source": [
    "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf41d2-c4f7-4aeb-acbe-56c64734cf80",
   "metadata": {},
   "source": [
    "In the context of building a predictive model for customer churn in a telecom company, the Filter Method for feature selection involves evaluating the relevance of individual features independently of the predictive model. Here's a step-by-step process for choosing the most pertinent attributes using the Filter Method:\n",
    "\n",
    "1. **Understand the Dataset:**\n",
    "   - Gain a thorough understanding of the dataset, including the available features, the target variable (customer churn), and any potential challenges or biases in the data.\n",
    "\n",
    "2. **Define the Objective:**\n",
    "   - Clearly define the objective of the predictive model. In the case of customer churn prediction, the goal is to identify features that have a significant impact on predicting whether a customer is likely to churn.\n",
    "\n",
    "3. **Explore Data Distribution:**\n",
    "   - Explore the distribution of individual features and the target variable. Use summary statistics, visualizations, and correlation matrices to understand the relationships between features and the target variable.\n",
    "\n",
    "4. **Statistical Tests:**\n",
    "   - Apply statistical tests to assess the statistical significance of the relationship between each feature and the target variable. Common statistical tests include t-tests, chi-square tests, or analysis of variance (ANOVA) depending on the type of data (numeric or categorical).\n",
    "\n",
    "5. **Correlation Analysis:**\n",
    "   - Conduct correlation analysis to identify features that are highly correlated with the target variable. Consider using metrics such as Pearson correlation for numeric features and point-biserial correlation for binary features.\n",
    "\n",
    "6. **Information Gain or Mutual Information:**\n",
    "   - For categorical target variables, calculate information gain or mutual information scores for each feature. These measures quantify the amount of information that a feature provides about the target variable.\n",
    "\n",
    "7. **Filtering Criteria:**\n",
    "   - Establish filtering criteria based on statistical significance or correlation strength. Features that meet the predefined criteria are considered relevant and selected for further analysis.\n",
    "\n",
    "8. **Feature Ranking:**\n",
    "   - Rank the features based on their relevance scores or statistical significance. This ranking provides insights into the relative importance of each feature in relation to the target variable.\n",
    "\n",
    "9. **Subset Selection:**\n",
    "   - Optionally, based on the predefined criteria or ranking, create a subset of the most pertinent attributes. This subset will serve as the input for building the predictive model.\n",
    "\n",
    "10. **Model Training:**\n",
    "    - Use the selected subset of features to train and evaluate predictive models for customer churn. This step involves building machine learning models (e.g., logistic regression, decision trees, or ensemble methods) to predict churn using the chosen features.\n",
    "\n",
    "11. **Evaluate Model Performance:**\n",
    "    - Assess the performance of the predictive model using metrics such as accuracy, precision, recall, and F1 score. Iterate on the feature selection process if necessary to improve model performance.\n",
    "\n",
    "By following these steps, the Filter Method helps identify and select the most pertinent attributes for predicting customer churn based on their individual relevance to the target variable. This method provides a quick and interpretable way to filter out irrelevant or redundant features before employing more computationally intensive methods, such as Wrapper or Embedded methods, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a7834-47b2-4f56-af5f-892d490db941",
   "metadata": {},
   "source": [
    "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f2425-7ec5-4ad6-9f81-b1d382843e88",
   "metadata": {},
   "source": [
    "In the context of predicting the outcome of a soccer match using a large dataset with many features, the Embedded Method for feature selection involves incorporating feature selection directly into the model training process. Here's how you would use the Embedded Method to select the most relevant features for the model:\n",
    "\n",
    "1. **Choose a Model with Embedded Feature Selection:**\n",
    "   - Select a machine learning algorithm that inherently performs feature selection as part of the training process. Common models with embedded feature selection capabilities include:\n",
    "     - **Lasso Regression (L1 Regularization):** Penalizes the absolute values of coefficients, encouraging sparsity and automatically selecting relevant features.\n",
    "     - **Decision Trees and Random Forests:** Tree-based models have built-in feature importance scores that can be used for feature selection.\n",
    "     - **Elastic Net Regression:** Combines L1 and L2 regularization, allowing for both feature selection and coefficient shrinkage.\n",
    "\n",
    "2. **Prepare the Dataset:**\n",
    "   - Preprocess the dataset, handling missing values, encoding categorical variables, and scaling numeric features if necessary. Split the dataset into training and testing sets.\n",
    "\n",
    "3. **Feature Scaling:**\n",
    "   - Depending on the chosen model, perform feature scaling if required. Some models, like Lasso Regression, are sensitive to the scale of features.\n",
    "\n",
    "4. **Train the Model:**\n",
    "   - Train the selected machine learning model using the training dataset. The model will automatically adjust the feature coefficients during training based on their importance for predicting the target variable (soccer match outcome).\n",
    "\n",
    "5. **Extract Feature Importance:**\n",
    "   - For models like Decision Trees or Random Forests, extract feature importance scores after training. These scores represent the contribution of each feature to the model's predictive performance.\n",
    "\n",
    "6. **Set Feature Importance Threshold:**\n",
    "   - Define a threshold for feature importance scores to determine which features are considered relevant. Features with importance scores above the threshold are retained, while others are considered less relevant and may be excluded.\n",
    "\n",
    "7. **Evaluate Model Performance:**\n",
    "   - Assess the performance of the predictive model using the selected subset of features. Use metrics such as accuracy, precision, recall, and F1 score to evaluate the model's ability to predict soccer match outcomes.\n",
    "\n",
    "8. **Iterate and Fine-Tune:**\n",
    "   - Iterate on the process by adjusting the feature importance threshold, trying different models, or experimenting with hyperparameter tuning to improve model performance further.\n",
    "\n",
    "By using the Embedded Method, the model automatically learns the relevance of features during the training process. This approach is advantageous as it considers feature importance in the context of the model's predictive task. It can be particularly useful when dealing with high-dimensional datasets, such as those containing player statistics and team rankings in a soccer match prediction project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae1530-24dc-4fc1-a894-6e003f662c63",
   "metadata": {},
   "source": [
    "Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96c0d7-6b3f-40ea-a844-257c904e0d7f",
   "metadata": {},
   "source": [
    "In the context of predicting the price of a house based on features like size, location, and age, the Wrapper Method for feature selection involves evaluating different subsets of features by training and testing models iteratively. Here's how you would use the Wrapper Method to select the best set of features:\n",
    "\n",
    "1. **Choose a Subset of Features:**\n",
    "   - Start with a subset of features (a combination of size, location, age, etc.) to form the initial feature set.\n",
    "\n",
    "2. **Select a Model:**\n",
    "   - Choose a performance evaluation metric and a machine learning model for training and testing. Common models used in the Wrapper Method include Linear Regression, Decision Trees, or other models depending on the dataset characteristics.\n",
    "\n",
    "3. **Train and Evaluate the Model:**\n",
    "   - Train the selected model using the chosen subset of features and evaluate its performance on a validation set using the chosen metric. The performance metric could be Mean Squared Error (MSE), R-squared, or any other relevant metric for regression tasks.\n",
    "\n",
    "4. **Feature Subset Evaluation:**\n",
    "   - Based on the model's performance, assess the importance and contribution of each feature in the subset. This evaluation might involve looking at coefficients in linear models, feature importance in tree-based models, or other relevant indicators.\n",
    "\n",
    "5. **Iterative Feature Selection:**\n",
    "   - Iteratively modify the feature subset, adding or removing features, and retrain the model. Continue this process until a stopping criterion is met (e.g., a predefined number of features, achieving optimal performance, or reaching a specific model complexity).\n",
    "\n",
    "6. **Cross-Validation:**\n",
    "   - To reduce the risk of overfitting to a specific subset of data, use cross-validation. Perform multiple train-test splits or k-fold cross-validation during each iteration, ensuring a more robust assessment of feature importance.\n",
    "\n",
    "7. **Optimal Feature Subset:**\n",
    "   - Identify the feature subset that yields the best model performance according to the chosen evaluation metric. This subset represents the set of features considered most important for predicting house prices.\n",
    "\n",
    "8. **Evaluate on Test Set:**\n",
    "   - Once the optimal feature subset is determined, evaluate the final model on an independent test set to assess its generalization performance.\n",
    "\n",
    "9. **Fine-Tuning:**\n",
    "   - If needed, fine-tune the model hyperparameters or consider other model variations to achieve the best overall performance.\n",
    "\n",
    "The Wrapper Method, including techniques like Forward Selection, Backward Elimination, or Recursive Feature Elimination (RFE), systematically explores different feature subsets to find the optimal combination for the predictive model. This approach can help ensure that the model is trained with the most relevant features, enhancing interpretability and potentially improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bee7f9-2b3e-4fdc-964d-d5c812d35f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
